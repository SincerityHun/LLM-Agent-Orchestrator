version: '3.8'

services:
  # Unified vLLM endpoint for Llama-3.2-1B with multiple LoRA adapters
  vllm-llama:
    image: sincerityhun/vllm:cu128-nightly
    container_name: vllm-llama
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ../LLM_pool/llama-1b-csqa:/csqa-adapter
      - ../LLM_pool/llama-1b-casehold:/casehold-adapter
      - ../LLM_pool/llama-1b-medqa2:/medqa-adapter
      - /mnt/ssd1/shjung/huggingface:/root/.cache/huggingface
    command:
      - vllm
      - serve
      - meta-llama/Llama-3.2-1B-Instruct
      - --port
      - "8000"
      - --gpu-memory-utilization
      - "0.8"
      - --enable-lora
      - --lora-modules
      - csqa-lora=/csqa-adapter # 32
      - casehold-lora=/casehold-adapter # 64
      - medqa-lora=/medqa-adapter # 128
      - --max-lora-rank
      - "128"
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]

  # vLLM endpoint for MathQA (Qwen-3B)
  vllm-qwen:
    image: sincerityhun/vllm:cu128-nightly
    container_name: vllm-qwen
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ../LLM_pool/qwen-3b-mathqa-2:/adapter
      - /mnt/ssd1/shjung/huggingface:/root/.cache/huggingface
    command:
      - vllm
      - serve
      - Qwen/Qwen2.5-3B-Instruct
      - --port
      - "8001"
      - --gpu-memory-utilization
      - "0.8"
      - --enable-lora
      - --lora-modules
      - mathqa-lora=/adapter
      - --max-lora-rank
      - "64"
    ports:
      - "8001:8001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]

  # Main orchestrator application
  # orchestrator:
  #   build:
  #     context: ..
  #     dockerfile: docker/Dockerfile
  #   container_name: llm-orchestrator
  #   depends_on:
  #     - vllm-llama
  #     - vllm-qwen
  #   environment:
  #     - VLLM_LLAMA_ENDPOINT=http://vllm-llama:8000/v1
  #     - VLLM_QWEN_ENDPOINT=http://vllm-qwen:8001/v1
  #   volumes:
  #     - ..:/app
  #   stdin_open: true
  #   tty: true
